{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e0b90a-bc7e-4d90-9d2b-474ff794d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Este archivo es idéntico al medallion_pipeline.py en la raíz.\n",
    "# Se deja en /app/notebooks para ser ejecutado dentro del contenedor jupyter-lab\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    current_date,\n",
    "    current_timestamp,\n",
    "    lit,\n",
    "    when,\n",
    "    concat_ws,\n",
    "    expr,\n",
    "    date_format,\n",
    ")\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "LOG = logging.getLogger(\"medallion\")\n",
    "\n",
    "\n",
    "def build_spark(app_name=\"medallion_pipeline\"):\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "    )\n",
    "    return configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "def ingest_to_bronze(spark, input_path, bronze_path):\n",
    "    LOG.info(\"Leyendo archivo de entrada: %s\", input_path)\n",
    "    df_raw = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(input_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f84427e-32b6-40b4-be45-90c99fc667dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (471515548.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport re\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Limpiar nombres de columnas (coincidir con el resto del pipeline)\n",
    "    import re\n",
    "\n",
    "    def clean_name(c: str) -> str:\n",
    "        c = c.strip()\n",
    "        c = re.sub(r\"[;:{}()\\n\\t\\r]\", \"\", c)\n",
    "        c = re.sub(r\"\\s+\", \"_\", c)\n",
    "        c = re.sub(r\"[^A-Za-z0-9_]\", \"\", c)\n",
    "        return c\n",
    "\n",
    "    cols_limpias = [clean_name(c) for c in df_raw.columns]\n",
    "    for old, new in zip(df_raw.columns, cols_limpias):\n",
    "        if old != new:\n",
    "            df_raw = df_raw.withColumnRenamed(old, new)\n",
    "\n",
    "    df_bronze = (\n",
    "        df_raw\n",
    "        .withColumn(\"_ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", lit(input_path))\n",
    "        .withColumn(\"_ingestion_date\", date_format(current_timestamp(), \"yyyy-MM-dd\"))\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo a Bronze: %s (append, mergeSchema=true)\", bronze_path)\n",
    "    (\n",
    "        df_bronze.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(bronze_path)\n",
    "    )\n",
    "    LOG.info(\"Ingesta completada. Registros ingestados: %d\", df_bronze.count())\n",
    "    return df_bronze.count()\n",
    "\n",
    "\n",
    "def apply_quality_checks(df, precio_col, fecha_col):\n",
    "    precio_not_null = col(precio_col).isNotNull()\n",
    "    precio_positive = col(precio_col).cast(\"double\") > 0\n",
    "    fecha_not_null = col(fecha_col).isNotNull()\n",
    "    fecha_not_future = col(fecha_col) <= current_date()\n",
    "\n",
    "    df_validated = (\n",
    "        df.withColumn(\"_quality_checks\", expr(\n",
    "            \"struct(\"\n",
    "            f\"({precio_not_null._jc.toString()}) as precio_not_null, \"\n",
    "            f\"({precio_positive._jc.toString()}) as precio_positive, \"\n",
    "            f\"({fecha_not_null._jc.toString()}) as fecha_not_null, \"\n",
    "            f\"({fecha_not_future._jc.toString()}) as fecha_not_future\"\n",
    "            \")\"\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_is_valid\",\n",
    "        col(\"_quality_checks.precio_not_null\")\n",
    "        & col(\"_quality_checks.precio_positive\")\n",
    "        & col(\"_quality_checks.fecha_not_null\")\n",
    "        & col(\"_quality_checks.fecha_not_future\"),\n",
    "    )\n",
    "\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_rejection_reasons\",\n",
    "        concat_ws(\n",
    "            \", \",\n",
    "            when(~col(\"_quality_checks.precio_not_null\"), lit(\"Precio Base nulo\")),\n",
    "            when(~col(\"_quality_checks.precio_positive\"), lit(\"Precio Base <= 0\")),\n",
    "            when(~col(\"_quality_checks.fecha_not_null\"), lit(\"Fecha de Firma nula\")),\n",
    "            when(~col(\"_quality_checks.fecha_not_future\"), lit(\"Fecha de Firma futura\")),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_rejection_code\",\n",
    "        when(~col(\"_quality_checks.fecha_not_null\"), lit(\"E001\"))\n",
    "        .when(~col(\"_quality_checks.fecha_not_future\"), lit(\"E002\"))\n",
    "        .when(~col(\"_quality_checks.precio_not_null\"), lit(\"E003\"))\n",
    "        .when(~col(\"_quality_checks.precio_positive\"), lit(\"E004\"))\n",
    "        .otherwise(lit(None)),\n",
    "    )\n",
    "\n",
    "    return df_validated\n",
    "\n",
    "\n",
    "def write_clean_and_quarantine(df_validated, silver_path, quarantine_path, pipeline_version=\"v1\", write_mode_clean=\"merge\", write_mode_quarantine=\"append\"):\n",
    "    df_clean = df_validated.filter(col(\"_is_valid\"))\n",
    "    df_quarantine = (\n",
    "        df_validated.filter(~col(\"_is_valid\"))\n",
    "        .withColumn(\"motivo_rechazo\", col(\"_rejection_reasons\"))\n",
    "        .withColumn(\"_validation_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_pipeline_version\", lit(pipeline_version))\n",
    "        .withColumn(\"_quarantine_id\", expr(\"uuid()\"))\n",
    "        .withColumn(\"_reprocess_attempts\", lit(0))\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo %d registros limpios a Silver: %s\", df_clean.count(), silver_path)\n",
    "    (\n",
    "        df_clean.drop(\"_quality_checks\", \"_is_valid\", \"_rejection_reasons\")\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(silver_path)\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo %d registros inválidos a Quarantine: %s\", df_quarantine.count(), quarantine_path)\n",
    "    (\n",
    "        df_quarantine\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(write_mode_quarantine)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(quarantine_path)\n",
    "    )\n",
    "\n",
    "    return df_clean.count(), df_quarantine.count()\n",
    "\n",
    "\n",
    "def reprocess_quarantine(spark, quarantine_path, silver_path, pipeline_version=\"v1\"):\n",
    "    LOG.info(\"Reprocesando registros en quarantine: %s\", quarantine_path)\n",
    "    try:\n",
    "        df_q = spark.read.format(\"delta\").load(quarantine_path)\n",
    "    except Exception as e:\n",
    "        LOG.error(\"No se puede leer quarantine: %s\", e)\n",
    "        return 0\n",
    "\n",
    "    if df_q.count() == 0:\n",
    "        LOG.info(\"No hay registros en quarantine para reprocesar.\")\n",
    "        return 0\n",
    "\n",
    "    cols = df_q.columns\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "    df_revalidated = apply_quality_checks(df_q, precio_col, fecha_col)\n",
    "\n",
    "    df_to_move = df_revalidated.filter(col(\"_is_valid\"))\n",
    "    moved = df_to_move.count()\n",
    "\n",
    "    if moved > 0:\n",
    "        LOG.info(\"Moviendo %d registros reprocesados a Silver\", moved)\n",
    "        (\n",
    "            df_to_move.drop(\"_quality_checks\", \"_is_valid\", \"_rejection_reasons\")\n",
    "            .write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            dt = DeltaTable.forPath(spark, quarantine_path)\n",
    "            spark_sql = (\n",
    "                \"MERGE INTO delta.`{qp}` q \"\n",
    "                \"USING (SELECT _quarantine_id as _qid FROM values) s \"\n",
    "                )\n",
    "            ids = [r._quarantine_id for r in df_to_move.select(\"_quarantine_id\").collect()]\n",
    "            if ids:\n",
    "                ids_list = \",\".join([f\"'{i}'\" for i in ids])\n",
    "                update_stmt = (\n",
    "                    f\"MERGE INTO delta.`{quarantine_path}` AS q \\n\"\n",
    "                    f\"USING (SELECT explode(array({ids_list})) as _quarantine_id) s ON q._quarantine_id = s._quarantine_id \\n\"\n",
    "                    f\"WHEN MATCHED THEN UPDATE SET q._reprocess_attempts = q._reprocess_attempts + 1, q._reprocessed_at = current_timestamp()\"\n",
    "                )\n",
    "                spark.sql(update_stmt)\n",
    "        except Exception as e:\n",
    "            LOG.warning(\"No se pudo actualizar attempts en quarantine: %s\", e)\n",
    "\n",
    "    LOG.info(\"Reprocess completado. Movidos: %d\", moved)\n",
    "    return moved\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Medallion pipeline with quality gate and quarantine\")\n",
    "    p.add_argument(\"--input-path\", required=False, help=\"Archivo de entrada (CSV)\")\n",
    "    p.add_argument(\"--lakehouse-root\", default=\"data/lakehouse\", help=\"Ruta raíz del lakehouse\")\n",
    "    p.add_argument(\"--pipeline-version\", default=\"v1\", help=\"Versión del pipeline\")\n",
    "    p.add_argument(\"--mode\", choices=[\"dev\", \"prod\"], default=\"dev\")\n",
    "    p.add_argument(\"--run-reprocess\", action=\"store_true\", help=\"Ejecutar reprocess sobre quarantine\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    args = parse_args()\n",
    "\n",
    "    spark = build_spark()\n",
    "\n",
    "    bronze_path = f\"{args.lakehouse_root}/bronze/secop\"\n",
    "    silver_path = f\"{args.lakehouse_root}/silver/secop\"\n",
    "    quarantine_path = f\"{args.lakehouse_root}/quarantine/secop_errors\"\n",
    "\n",
    "    if args.input_path:\n",
    "        ingest_to_bronze(spark, args.input_path, bronze_path)\n",
    "\n",
    "    try:\n",
    "        df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "    except Exception as e:\n",
    "        LOG.error(\"No se pudo leer Bronze: %s\", e)\n",
    "        return\n",
    "\n",
    "    cols = df_bronze.columns\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "    df_validated = apply_quality_checks(df_bronze, precio_col, fecha_col)\n",
    "\n",
    "    clean_count, quarantine_count = write_clean_and_quarantine(\n",
    "        df_validated, silver_path, quarantine_path, pipeline_version=args.pipeline_version\n",
    "    )\n",
    "\n",
    "    total = df_bronze.count()\n",
    "    LOG.info(\"Resumen: total=%d, validos=%d, rechazados=%d\", total, clean_count, quarantine_count)\n",
    "\n",
    "    reject_pct = 100.0 * quarantine_count / total if total > 0 else 0\n",
    "    if reject_pct > 5 and args.mode == \"prod\":\n",
    "        LOG.warning(\"ALERTA: Tasa de rechazo > 5%% (%.2f%%).\", reject_pct)\n",
    "\n",
    "    if args.run_reprocess:\n",
    "        moved = reprocess_quarantine(spark, quarantine_path, silver_path, args.pipeline_version)\n",
    "        LOG.info(\"Registros reprocesados y movidos: %d\", moved)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455e0ad-c307-4347-9bcb-a38215c62d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
