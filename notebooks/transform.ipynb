{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecfd896-51d7-4a2b-aabc-7db5379ae764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-01db2c56-d9e8-4d85-8139-f5ca169f2fb2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 253ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-01db2c56-d9e8-4d85-8139-f5ca169f2fb2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "26/02/02 05:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/02 05:28:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos de Bronze de prueba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 05:28:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "26/02/02 05:28:42 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 3:===================================================>     (45 + 4) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total registros en Bronze: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# %% Test: Transformación con validación de datos inválidos\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from delta import *\n",
    "\n",
    "spark = (\n",
    "    configure_spark_with_delta_pip(\n",
    "        SparkSession.builder\n",
    "        .appName(\"Lab_SECOP_TEST_Transform\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "bronze_path = \"data/lakehouse/bronze/test_secop\"\n",
    "\n",
    "print(\"Leyendo datos de Bronze de prueba...\")\n",
    "df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "total_bronze = df_bronze.count()\n",
    "print(f\"Total registros en Bronze: {total_bronze}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52d3556-79dd-4dba-8f08-dd23ebde062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando columna de precio: Precio_Base\n",
      "Usando columna de fecha: Fecha_de_Firma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas\n",
    "cols = df_bronze.columns\n",
    "\n",
    "precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "print(f\"Usando columna de precio: {precio_col}\")\n",
    "print(f\"Usando columna de fecha: {fecha_col}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cf0ac9-3a78-4976-bb2f-9c12ed5c6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de calidad\n",
    "cond_precio_ok = (col(precio_col).isNotNull()) & (col(precio_col).cast(\"double\") > 0)\n",
    "cond_fecha_ok = col(fecha_col).isNotNull()\n",
    "\n",
    "cond_registro_valido = cond_precio_ok & cond_fecha_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d7620e-fb5a-4daf-aa55-a8dac693a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de registros válidos\n",
    "df_validos = df_bronze.filter(cond_registro_valido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba7a580-17ca-4af9-8a01-8fa273143a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registros válidos (SILVER): 6\n",
      "❌ Registros inválidos (QUARANTINE): 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame de registros inválidos con motivo_rechazo\n",
    "df_invalidos = (\n",
    "    df_bronze\n",
    "    .filter(~cond_registro_valido)\n",
    "    .withColumn(\n",
    "        \"motivo_rechazo\",\n",
    "        when(col(fecha_col).isNull(), lit(\"Fecha de Firma nula\"))\n",
    "        .when(col(precio_col).isNull(), lit(\"Precio Base nulo\"))\n",
    "        .when(col(precio_col).cast(\"double\") <= 0, lit(\"Precio Base <= 0\"))\n",
    "        .otherwise(lit(\"Incumple reglas de calidad\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"✅ Registros válidos (SILVER): {df_validos.count()}\")\n",
    "print(f\"❌ Registros inválidos (QUARANTINE): {df_invalidos.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac754f7-3fab-4246-85ce-b7739122ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REGISTROS INVÁLIDOS CAPTURADOS:\n",
      "================================================================================\n",
      "+-------------+-----------+--------------+-------------------+\n",
      "|Entidad      |Precio_Base|Fecha_de_Firma|motivo_rechazo     |\n",
      "+-------------+-----------+--------------+-------------------+\n",
      "|HOSPITAL 3   |NULL       |NULL          |Fecha de Firma nula|\n",
      "|UNIVERSIDAD 4|-100       |2023-04-10    |Precio Base <= 0   |\n",
      "|MUNICIPIO 6  |NULL       |NULL          |Fecha de Firma nula|\n",
      "|EMPRESA 8    |NULL       |NULL          |Fecha de Firma nula|\n",
      "+-------------+-----------+--------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Mostrar los registros inválidos\n",
    "if df_invalidos.count() > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REGISTROS INVÁLIDOS CAPTURADOS:\")\n",
    "    print(\"=\" * 80)\n",
    "    df_invalidos.select(\"Entidad\", \"Precio_Base\", \"Fecha_de_Firma\", \"motivo_rechazo\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461cc3c3-ac0c-4c5d-8a2f-edfcbe48c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escribiendo registros VÁLIDOS en: data/lakehouse/silver/test_secop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribiendo registros INVÁLIDOS en: data/lakehouse/quarantine/test_secop_errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transformación de prueba completada.\n",
      "   - Silver (válidos): 6 registros\n",
      "   - Quarantine (inválidos): 4 registros\n"
     ]
    }
   ],
   "source": [
    "# Escritura\n",
    "silver_path = \"data/lakehouse/silver/test_secop\"\n",
    "quarantine_path = \"data/lakehouse/quarantine/test_secop_errors\"\n",
    "\n",
    "print(f\"\\nEscribiendo registros VÁLIDOS en: {silver_path}\")\n",
    "(\n",
    "    df_validos.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(silver_path)\n",
    ")\n",
    "\n",
    "print(f\"Escribiendo registros INVÁLIDOS en: {quarantine_path}\")\n",
    "(\n",
    "    df_invalidos.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(quarantine_path)\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Transformación de prueba completada.\")\n",
    "print(f\"   - Silver (válidos): {df_validos.count()} registros\")\n",
    "print(f\"   - Quarantine (inválidos): {df_invalidos.count()} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22b346-e24d-49f8-8a67-ae256836b097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
