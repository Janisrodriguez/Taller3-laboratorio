{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27be5d4d-51f2-4fda-9775-9ff520da9c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input-path INPUT_PATH]\n",
      "                             [--lakehouse-root LAKEHOUSE_ROOT]\n",
      "                             [--pipeline-version PIPELINE_VERSION]\n",
      "                             [--mode {dev,prod}] [--run-reprocess]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-fa529a8c-c3d0-4e11-a7f1-06c0682f9923.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3709: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    current_date,\n",
    "    current_timestamp,\n",
    "    lit,\n",
    "    when,\n",
    "    concat_ws,\n",
    "    expr,\n",
    "    date_format,\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "LOG = logging.getLogger(\"medallion\")\n",
    "\n",
    "def clean_name(c: str) -> str:\n",
    "    c = c.strip()\n",
    "    c = re.sub(r\"[;:{}()\\n\\t\\r]\", \"\", c)\n",
    "    c = re.sub(r\"\\s+\", \"_\", c)\n",
    "    c = re.sub(r\"[^A-Za-z0-9_]\", \"\", c)\n",
    "    return c\n",
    "\n",
    "\n",
    "def build_spark(app_name=\"medallion_pipeline\"):\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "    )\n",
    "    return configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "def ingest_to_bronze(spark, input_path, bronze_path):\n",
    "    LOG.info(\"Leyendo archivo de entrada: %s\", input_path)\n",
    "    df_raw = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(input_path)\n",
    "    )\n",
    "\n",
    "    # Limpiar nombres de columnas (coincidir con el resto del pipeline)\n",
    "    cols_limpias = [clean_name(c) for c in df_raw.columns]\n",
    "    for old, new in zip(df_raw.columns, cols_limpias):\n",
    "        if old != new:\n",
    "            df_raw = df_raw.withColumnRenamed(old, new)\n",
    "\n",
    "    # Añadir metadata de ingest\n",
    "    df_bronze = (\n",
    "        df_raw\n",
    "        .withColumn(\"_ingestion_time\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", lit(input_path))\n",
    "        .withColumn(\"_ingestion_date\", date_format(current_timestamp(), \"yyyy-MM-dd\"))\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo a Bronze: %s (append, mergeSchema=true)\", bronze_path)\n",
    "    (\n",
    "        df_bronze.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(bronze_path)\n",
    "    )\n",
    "    LOG.info(\"Ingesta completada. Registros ingestados: %d\", df_bronze.count())\n",
    "    return df_bronze.count()\n",
    "\n",
    "\n",
    "def apply_quality_checks(df, precio_col, fecha_col):\n",
    "    # Reglas (según Sección 11):\n",
    "    # - Precio Base no nulo y > 0\n",
    "    # - Fecha de Firma no nula y no futura\n",
    "\n",
    "    precio_not_null = col(precio_col).isNotNull()\n",
    "    precio_positive = col(precio_col).cast(\"double\") > 0\n",
    "    fecha_not_null = col(fecha_col).isNotNull()\n",
    "    fecha_not_future = col(fecha_col) <= current_date()\n",
    "\n",
    "    # Struct con checks (útil para reporting)\n",
    "    df_validated = (\n",
    "        df.withColumn(\"_quality_checks\", expr(\n",
    "            \"struct(\"\n",
    "            f\"({precio_not_null._jc.toString()}) as precio_not_null, \"\n",
    "            f\"({precio_positive._jc.toString()}) as precio_positive, \"\n",
    "            f\"({fecha_not_null._jc.toString()}) as fecha_not_null, \"\n",
    "            f\"({fecha_not_future._jc.toString()}) as fecha_not_future\"\n",
    "            \")\"\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    # _is_valid\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_is_valid\",\n",
    "        col(\"_quality_checks.precio_not_null\")\n",
    "        & col(\"_quality_checks.precio_positive\")\n",
    "        & col(\"_quality_checks.fecha_not_null\")\n",
    "        & col(\"_quality_checks.fecha_not_future\"),\n",
    "    )\n",
    "\n",
    "    # Motivo de rechazo (concat de razones)\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_rejection_reasons\",\n",
    "        concat_ws(\n",
    "            \", \",\n",
    "            when(~col(\"_quality_checks.precio_not_null\"), lit(\"Precio Base nulo\")),\n",
    "            when(~col(\"_quality_checks.precio_positive\"), lit(\"Precio Base <= 0\")),\n",
    "            when(~col(\"_quality_checks.fecha_not_null\"), lit(\"Fecha de Firma nula\")),\n",
    "            when(~col(\"_quality_checks.fecha_not_future\"), lit(\"Fecha de Firma futura\")),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Rejection code prioritario\n",
    "    df_validated = df_validated.withColumn(\n",
    "        \"_rejection_code\",\n",
    "        when(~col(\"_quality_checks.fecha_not_null\"), lit(\"E001\"))\n",
    "        .when(~col(\"_quality_checks.fecha_not_future\"), lit(\"E002\"))\n",
    "        .when(~col(\"_quality_checks.precio_not_null\"), lit(\"E003\"))\n",
    "        .when(~col(\"_quality_checks.precio_positive\"), lit(\"E004\"))\n",
    "        .otherwise(lit(None)),\n",
    "    )\n",
    "\n",
    "    return df_validated\n",
    "\n",
    "\n",
    "def write_clean_and_quarantine(df_validated, silver_path, quarantine_path, pipeline_version=\"v1\", write_mode_clean=\"merge\", write_mode_quarantine=\"append\"):\n",
    "    # Split\n",
    "    df_clean = df_validated.filter(col(\"_is_valid\"))\n",
    "    df_quarantine = (\n",
    "        df_validated.filter(~col(\"_is_valid\"))\n",
    "        .withColumn(\"motivo_rechazo\", col(\"_rejection_reasons\"))\n",
    "        .withColumn(\"_validation_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_pipeline_version\", lit(pipeline_version))\n",
    "        .withColumn(\"_quarantine_id\", expr(\"uuid()\"))\n",
    "        .withColumn(\"_reprocess_attempts\", lit(0))\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo %d registros limpios a Silver: %s\", df_clean.count(), silver_path)\n",
    "    (\n",
    "        df_clean.drop(\"_quality_checks\", \"_is_valid\", \"_rejection_reasons\")\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(silver_path)\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Escribiendo %d registros inválidos a Quarantine: %s\", df_quarantine.count(), quarantine_path)\n",
    "    (\n",
    "        df_quarantine\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(write_mode_quarantine)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(quarantine_path)\n",
    "    )\n",
    "\n",
    "    return df_clean.count(), df_quarantine.count()\n",
    "\n",
    "\n",
    "def reprocess_quarantine(spark, quarantine_path, silver_path, pipeline_version=\"v1\"):\n",
    "    LOG.info(\"Reprocesando registros en quarantine: %s\", quarantine_path)\n",
    "    try:\n",
    "        df_q = spark.read.format(\"delta\").load(quarantine_path)\n",
    "    except Exception as e:\n",
    "        LOG.error(\"No se puede leer quarantine: %s\", e)\n",
    "        return 0\n",
    "\n",
    "    if df_q.count() == 0:\n",
    "        LOG.info(\"No hay registros en quarantine para reprocesar.\")\n",
    "        return 0\n",
    "\n",
    "    # Re-aplicar validaciones (suponemos mismas columnas)\n",
    "    cols = df_q.columns\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "    df_revalidated = apply_quality_checks(df_q, precio_col, fecha_col)\n",
    "\n",
    "    df_to_move = df_revalidated.filter(col(\"_is_valid\"))\n",
    "    moved = df_to_move.count()\n",
    "\n",
    "    if moved > 0:\n",
    "        LOG.info(\"Moviendo %d registros reprocesados a Silver\", moved)\n",
    "        (\n",
    "            df_to_move.drop(\"_quality_checks\", \"_is_valid\", \"_rejection_reasons\")\n",
    "            .write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "        )\n",
    "\n",
    "        # Actualizar quarantine: incrementar attempts y marcar _reprocessed_at\n",
    "        try:\n",
    "            dt = DeltaTable.forPath(spark, quarantine_path)\n",
    "            spark_sql = (\n",
    "                \"MERGE INTO delta.`{qp}` q \"\n",
    "                \"USING (SELECT _quarantine_id as _qid FROM values) s \"\n",
    "                )\n",
    "            # Usamos condición IN con lista de ids\n",
    "            ids = [r._quarantine_id for r in df_to_move.select(\"_quarantine_id\").collect()]\n",
    "            if ids:\n",
    "                ids_list = \",\".join([f\"'{i}'\" for i in ids])\n",
    "                update_stmt = (\n",
    "                    f\"MERGE INTO delta.`{quarantine_path}` AS q \\n\"\n",
    "                    f\"USING (SELECT explode(array({ids_list})) as _quarantine_id) s ON q._quarantine_id = s._quarantine_id \\n\"\n",
    "                    f\"WHEN MATCHED THEN UPDATE SET q._reprocess_attempts = q._reprocess_attempts + 1, q._reprocessed_at = current_timestamp()\"\n",
    "                )\n",
    "                spark.sql(update_stmt)\n",
    "        except Exception as e:\n",
    "            LOG.warning(\"No se pudo actualizar attempts en quarantine: %s\", e)\n",
    "\n",
    "    LOG.info(\"Reprocess completado. Movidos: %d\", moved)\n",
    "    return moved\n",
    "\n",
    "\n",
    "def notebook_debug_example():\n",
    "    \"\"\"Helper para usar desde Jupyter: crea un DataFrame de ejemplo y demuestra\n",
    "    la limpieza de nombres de columna y la validación sin necesidad de ejecutar\n",
    "    fragmentos indentados en la celda (evita IndentationError).\n",
    "\n",
    "    Uso en notebook:\n",
    "      from medallion_pipeline import notebook_debug_example\n",
    "      notebook_debug_example()\n",
    "    \"\"\"\n",
    "    spark = build_spark(app_name=\"medallion_debug\")\n",
    "\n",
    "    # Crear un DataFrame de ejemplo con nombres con espacios y valores inválidos\n",
    "    data = [\n",
    "        (\"ALCALDIA 1\", None, \"ANTIOQUIA\", None, None, None, None, None, None, \"2023-01-01\", None, None, 3582652, None, None),\n",
    "        (\"MUNICIPIO 6\", None, \"NARIÑO\", None, None, None, None, None, None, None, None, None, None, None, None),\n",
    "        (\"UNIVERSIDAD 4\", None, \"VALLE\", None, None, None, None, None, None, \"2023-04-10\", None, None, -100, None, None),\n",
    "    ]\n",
    "    cols = [\n",
    "        \"Entidad\",\"Nit Entidad\",\"Departamento\",\"Ciudad\",\"Estado\",\"Descripcion del Proceso\",\n",
    "        \"Tipo de Contrato\",\"Modalidad de Contratacion\",\"Justificacion Modalidad de Contratacion\",\n",
    "        \"Fecha de Firma\",\"Fecha de Inicio del Contrato\",\"Fecha de Fin del Contrato\",\"Precio Base\",\"Valor Total\",\"Valor Pagado\"\n",
    "    ]\n",
    "\n",
    "    # Schema explícito para evitar inferencia fallida cuando columnas son todas NULL\n",
    "    schema = StructType([\n",
    "        StructField(\"Entidad\", StringType(), True),\n",
    "        StructField(\"Nit Entidad\", StringType(), True),\n",
    "        StructField(\"Departamento\", StringType(), True),\n",
    "        StructField(\"Ciudad\", StringType(), True),\n",
    "        StructField(\"Estado\", StringType(), True),\n",
    "        StructField(\"Descripcion del Proceso\", StringType(), True),\n",
    "        StructField(\"Tipo de Contrato\", StringType(), True),\n",
    "        StructField(\"Modalidad de Contratacion\", StringType(), True),\n",
    "        StructField(\"Justificacion Modalidad de Contratacion\", StringType(), True),\n",
    "        StructField(\"Fecha de Firma\", StringType(), True),\n",
    "        StructField(\"Fecha de Inicio del Contrato\", StringType(), True),\n",
    "        StructField(\"Fecha de Fin del Contrato\", StringType(), True),\n",
    "        StructField(\"Precio Base\", DoubleType(), True),\n",
    "        StructField(\"Valor Total\", DoubleType(), True),\n",
    "        StructField(\"Valor Pagado\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "    df_raw = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    print(\"Columnas antes:\", df_raw.columns)\n",
    "\n",
    "    # Renombrado usando clean_name (no hay indentación en la celda)\n",
    "    cols_limpias = [clean_name(c) for c in df_raw.columns]\n",
    "    for old, new in zip(df_raw.columns, cols_limpias):\n",
    "        if old != new:\n",
    "            df_raw = df_raw.withColumnRenamed(old, new)\n",
    "\n",
    "    print(\"Columnas después:\", df_raw.columns)\n",
    "\n",
    "    # Aplicar validaciones y mostrar registros inválidos\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in df_raw.columns else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in df_raw.columns else \"Fecha de Firma\"\n",
    "    df_validated = apply_quality_checks(df_raw, precio_col, fecha_col)\n",
    "\n",
    "    print(\"Registros inválidos (motivo):\")\n",
    "    df_validated.filter(~col(\"_is_valid\")).select(\"Entidad\", \"Precio_Base\", \"Fecha_de_Firma\", \"_rejection_code\", \"_rejection_reasons\").show(truncate=False)\n",
    "\n",
    "\n",
    "def run_pipeline_interactive(\n",
    "    input_path=None,\n",
    "    lakehouse_root=\"data/lakehouse\",\n",
    "    pipeline_version=\"v1\",\n",
    "    run_reprocess=False,\n",
    "    mode=\"dev\",\n",
    "    spark=None,\n",
    "):\n",
    "    \"\"\"Ejecuta el pipeline desde un notebook sin pegar código indentado.\n",
    "\n",
    "    Retorna un diccionario con métricas básicas y paths.\n",
    "    \"\"\"\n",
    "    local_spark = False\n",
    "    if spark is None:\n",
    "        spark = build_spark(app_name=\"medallion_interactive\")\n",
    "        local_spark = True\n",
    "\n",
    "    bronze_path = f\"{lakehouse_root}/bronze/secop\"\n",
    "    silver_path = f\"{lakehouse_root}/silver/secop\"\n",
    "    quarantine_path = f\"{lakehouse_root}/quarantine/secop_errors\"\n",
    "\n",
    "    if input_path:\n",
    "        ingest_to_bronze(spark, input_path, bronze_path)\n",
    "\n",
    "    try:\n",
    "        df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "    except Exception as e:\n",
    "        LOG.error(\"No se pudo leer Bronze: %s\", e)\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    cols = df_bronze.columns\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "    df_validated = apply_quality_checks(df_bronze, precio_col, fecha_col)\n",
    "\n",
    "    clean_count, quarantine_count = write_clean_and_quarantine(\n",
    "        df_validated, silver_path, quarantine_path, pipeline_version=pipeline_version\n",
    "    )\n",
    "\n",
    "    total = df_bronze.count()\n",
    "    summary = {\n",
    "        \"total\": total,\n",
    "        \"clean\": clean_count,\n",
    "        \"quarantine\": quarantine_count,\n",
    "        \"reject_pct\": (100.0 * quarantine_count / total if total > 0 else 0),\n",
    "    }\n",
    "\n",
    "    LOG.info(\"Resumen interactivo: %s\", summary)\n",
    "\n",
    "    if summary[\"reject_pct\"] > 5 and mode == \"prod\":\n",
    "        LOG.warning(\"ALERTA: Tasa de rechazo > 5%% (%.2f%%).\", summary[\"reject_pct\"])\n",
    "\n",
    "    if run_reprocess:\n",
    "        moved = reprocess_quarantine(spark, quarantine_path, silver_path, pipeline_version)\n",
    "        summary[\"moved\"] = moved\n",
    "        LOG.info(\"Registros reprocesados y movidos: %d\", moved)\n",
    "\n",
    "    if local_spark:\n",
    "        spark.stop()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Medallion pipeline with quality gate and quarantine\")\n",
    "    p.add_argument(\"--input-path\", required=False, help=\"Archivo de entrada (CSV)\")\n",
    "    p.add_argument(\"--lakehouse-root\", default=\"data/lakehouse\", help=\"Ruta raíz del lakehouse\")\n",
    "    p.add_argument(\"--pipeline-version\", default=\"v1\", help=\"Versión del pipeline\")\n",
    "    p.add_argument(\"--mode\", choices=[\"dev\", \"prod\"], default=\"dev\")\n",
    "    p.add_argument(\"--run-reprocess\", action=\"store_true\", help=\"Ejecutar reprocess sobre quarantine\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    args = parse_args()\n",
    "\n",
    "    spark = build_spark()\n",
    "\n",
    "    bronze_path = f\"{args.lakehouse_root}/bronze/secop\"\n",
    "    silver_path = f\"{args.lakehouse_root}/silver/secop\"\n",
    "    quarantine_path = f\"{args.lakehouse_root}/quarantine/secop_errors\"\n",
    "\n",
    "    if args.input_path:\n",
    "        ingest_to_bronze(spark, args.input_path, bronze_path)\n",
    "\n",
    "    # Leer Bronze más reciente\n",
    "    try:\n",
    "        df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "    except Exception as e:\n",
    "        LOG.error(\"No se pudo leer Bronze: %s\", e)\n",
    "        return\n",
    "\n",
    "    cols = df_bronze.columns\n",
    "    precio_col = \"Precio_Base\" if \"Precio_Base\" in cols else \"Precio Base\"\n",
    "    fecha_col = \"Fecha_de_Firma\" if \"Fecha_de_Firma\" in cols else \"Fecha de Firma\"\n",
    "\n",
    "    df_validated = apply_quality_checks(df_bronze, precio_col, fecha_col)\n",
    "\n",
    "    clean_count, quarantine_count = write_clean_and_quarantine(\n",
    "        df_validated, silver_path, quarantine_path, pipeline_version=args.pipeline_version\n",
    "    )\n",
    "\n",
    "    total = df_bronze.count()\n",
    "    LOG.info(\"Resumen: total=%d, validos=%d, rechazados=%d\", total, clean_count, quarantine_count)\n",
    "\n",
    "    # Alerta simple\n",
    "    reject_pct = 100.0 * quarantine_count / total if total > 0 else 0\n",
    "    if reject_pct > 5 and args.mode == \"prod\":\n",
    "        LOG.warning(\"ALERTA: Tasa de rechazo > 5%% (%.2f%%).\", reject_pct)\n",
    "\n",
    "    if args.run_reprocess:\n",
    "        moved = reprocess_quarantine(spark, quarantine_path, silver_path, args.pipeline_version)\n",
    "        LOG.info(\"Registros reprocesados y movidos: %d\", moved)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce6bda-a3c6-4158-b913-262313b0ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
